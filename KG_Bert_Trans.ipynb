{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d94Zg5crQH_k",
        "outputId": "16353b6b-14bb-4d76-d0ce-d271a8bfe9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruR3ujFPQAKe",
        "outputId": "825c0672-901f-4517-c7d3-c50a1cb0cbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: how many caps Diego Costa have?\n",
            "Answer: 31\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "knowledge_graph = {}\n",
        "\n",
        "# List of knowledge graph files\n",
        "knowledge_graph_files = ['knowledge_graph.txt', '2.txt', '3.txt', '4.txt', '5.txt', '6.txt', '7.txt', '8.txt']\n",
        "\n",
        "# Function to read and merge knowledge graph data from multiple files\n",
        "def read_and_merge_knowledge_graph(files):\n",
        "    merged_graph = {}\n",
        "    for file in files:\n",
        "        with open(file, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip().split('\\t')\n",
        "                entity = line[0]\n",
        "                attribute = line[1]\n",
        "                value = line[2]\n",
        "\n",
        "                if entity not in merged_graph:\n",
        "                    merged_graph[entity] = {}\n",
        "                merged_graph[entity][attribute] = value\n",
        "    return merged_graph\n",
        "\n",
        "knowledge_graph = read_and_merge_knowledge_graph(knowledge_graph_files)\n",
        "\n",
        "# Load the BERT-based question-answering model and tokenizer\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def answer_question(question):\n",
        "    # Process the question with spaCy\n",
        "    doc = nlp(question)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    for entity, entity_type in entities:\n",
        "        if entity in knowledge_graph:\n",
        "            attributes = knowledge_graph[entity]\n",
        "            for attribute, value in attributes.items():\n",
        "                if attribute.lower() in question.lower():\n",
        "                    return value\n",
        "\n",
        "    # Check if the question is about the age of a player\n",
        "    if \"age\" in question.lower() or \"how old\" in question.lower():\n",
        "        for entity, attributes in knowledge_graph.items():\n",
        "            if \"date of birth\" in attributes:\n",
        "                dob = attributes[\"date of birth\"]\n",
        "\n",
        "                # Calculate age based on date of birth\n",
        "                year_of_birth = int(dob.split(\"-\")[0])\n",
        "                current_year = 2023\n",
        "                age = current_year - year_of_birth\n",
        "\n",
        "                return f\"{entity} is {age} years old.\"\n",
        "\n",
        "    # Check if the question is about the youngest or oldest player (optional)\n",
        "    if \"youngest\" in question.lower() or \"oldest\" in question.lower():\n",
        "        target_attribute = \"date of birth\"\n",
        "        target_player = None\n",
        "        target_value = float('inf') if \"youngest\" in question.lower() else 0\n",
        "\n",
        "        for entity, attributes in knowledge_graph.items():\n",
        "            if target_attribute in attributes and \"jersey\" in attributes:\n",
        "                dob = attributes[target_attribute]\n",
        "                jersey = attributes[\"jersey\"]\n",
        "\n",
        "                # Calculate age based on date of birth\n",
        "                year_of_birth = int(dob.split(\"-\")[0])\n",
        "                current_year = 2023\n",
        "                age = current_year - year_of_birth\n",
        "\n",
        "                if \"youngest\" in question.lower():\n",
        "                    if age < target_value:\n",
        "                        target_value = age\n",
        "                        target_player = entity\n",
        "                elif \"oldest\" in question.lower():\n",
        "                    if age > target_value:\n",
        "                        target_value = age\n",
        "                        target_player = entity\n",
        "\n",
        "        if target_player:\n",
        "            return target_player\n",
        "\n",
        "    # If the question doesn't match any of the conditions above, use the BERT-based model\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    start_positions, end_positions = model(**inputs)\n",
        "    start_index = torch.argmax(start_positions)\n",
        "    end_index = torch.argmax(end_positions)\n",
        "    answer = text[start_index:end_index+1]\n",
        "\n",
        "    return answer\n",
        "\n",
        "question = \"how many caps Diego Costa have?\"\n",
        "text = \"\"  # You should provide the context text here\n",
        "answer = answer_question(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nS-FwPY0S7a7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}