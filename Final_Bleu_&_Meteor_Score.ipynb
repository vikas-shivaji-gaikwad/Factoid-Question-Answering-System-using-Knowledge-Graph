{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgtk-snubpw_",
        "outputId": "110bebde-5974-4138-d242-313bb6c2c49d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOWlqypAbo3F",
        "outputId": "f68273ae-36a2-4f5e-8558-aea1f7001648"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Read the CSV files into dataframes\n",
        "references_df = pd.read_csv('original_answer.csv')\n",
        "candidates_df = pd.read_csv('predicted_kg_response.csv')\n",
        "\n",
        "# Make sure the \"Sentence ID\" columns match in both dataframes\n",
        "\n",
        "# Initialize lists to store reference and candidate sentences\n",
        "references = references_df['original_answer'].tolist()\n",
        "candidates = candidates_df['predicted_kg_response'].tolist()\n",
        "\n",
        "# Tokenize the candidate sentences\n",
        "tokenized_candidates = [word_tokenize(cand) for cand in candidates]\n",
        "\n",
        "# Tokenize the reference sentences\n",
        "tokenized_references = [word_tokenize(ref) for ref in references]\n",
        "\n",
        "# Calculate BLEU score for a single sentence\n",
        "bleu_scores = [sentence_bleu([ref], cand) for ref, cand in zip(tokenized_references, tokenized_candidates)]\n",
        "\n",
        "print(\"BLEU Scores:\", bleu_scores)\n",
        "\n",
        "# Calculate BLEU score for the corpus\n",
        "corpus_bleu_score = corpus_bleu([[ref] for ref in tokenized_references], [cand for cand in tokenized_candidates])\n",
        "\n",
        "print(\"Corpus BLEU Score:\", corpus_bleu_score)\n",
        "\n",
        "# Calculate METEOR scores for the entire corpus\n",
        "meteor_scores = [meteor_score([ref], cand) for ref, cand in zip(tokenized_references, tokenized_candidates)]\n",
        "\n",
        "print(\"METEOR Scores:\", meteor_scores)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
